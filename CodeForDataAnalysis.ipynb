{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4978af-1138-424e-9a9b-fb69ce229a48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#installations for necessary packages   \n",
    "\"\"\"\n",
    "!pip install cdsapi\n",
    "!pip install shutil\n",
    "!pip install zipfile\n",
    "!pip install xarray(complete)\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install cartopy \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a4d69-9a62-4160-844d-98abb9251067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import shutil\n",
    "import zipfile\n",
    "import os \n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429a7cac-06fc-4ec0-a6f1-219b16286132",
   "metadata": {},
   "source": [
    "Installing CDS API\n",
    "\n",
    "If you are not familiar with the Copernicus Climate Data Store API, please view the following step by step guide for installation: \n",
    "Windows: https://confluence.ecmwf.int/display/CKB/How+to+install+and+use+CDS+API+on+Windows\n",
    "MacOS: https://confluence.ecmwf.int/display/CKB/How+to+install+and+use+CDS+API+on+macOS\n",
    "Linux: https://cds.climate.copernicus.eu/api-how-to\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc688559-9c73-4d43-86db-81ed8d8c86dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#must set up Copernicus Climate Data Store API before running code\n",
    "c = cdsapi.Client()          \n",
    "months = ['01', '02','03', '04', '05', '06', '07', '08', '09', '10', '11', '12']  #months of the year\n",
    "destination_dir = 'SSTdata'   #specify directory for where nc files will go\n",
    "\n",
    "for month in months:\n",
    "    os.makedirs(destination_dir, exist_ok = True)    #make directory\n",
    "    \n",
    "    c.retrieve(\n",
    "        'satellite-sea-surface-temperature',\n",
    "        {\n",
    "            'version': '2_1',\n",
    "            'variable': 'all',\n",
    "            'format': 'zip',\n",
    "            'processinglevel': 'level_4',\n",
    "            'year': [\n",
    "                '1997', '1998', '1999',\n",
    "                '2000', '2001', '2002',\n",
    "                '2003', '2004', '2005',\n",
    "                '2006', '2007', '2008',\n",
    "                '2009', '2010',\n",
    "            ],\n",
    "            'month': month,\n",
    "            'day': [\n",
    "                '01', '04', '07',\n",
    "                '10', '13', '16',\n",
    "                '19', '22', '25',\n",
    "                '28',\n",
    "            ],\n",
    "            'sensor_on_satellite': 'combined_product',\n",
    "        },\n",
    "        f\"SSTmonth{month}.zip\")   #download global SST for every three days while looping through each month. \n",
    "    \n",
    "    shutil.move(f\"SSTmonth{month}.zip\", destination_dir)       \n",
    "    with zipfile.ZipFile(f\"{destination_dir}/SSTmonth{month}.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(destination_dir)     #move files to directory and extract zip files into directory\n",
    "        \n",
    "    if os.path.isfile(f\"{destination_dir}/SSTmonth{month}.zip\"):\n",
    "        os.remove(f\"{destination_dir}/SSTmonth{month}.zip\")      #delete zip files\n",
    "        \n",
    "    print(f\"Download for month {month} complete\")\n",
    "print('All downloads complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d7df5-40f6-4baa-8439-d7d8eec997dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This code will download all the wind data for later processing. Already selected for the study area\n",
    "\n",
    "years = [1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, \n",
    "        2007, 2008, 2009, 2010]     #For this request, must loop through years.\n",
    "\n",
    "destination_dir = 'WINDdata'   #specify directory for where nc files will go\n",
    "\n",
    "c = cdsapi.Client()\n",
    "for year in years:\n",
    "    c.retrieve(\n",
    "        'reanalysis-era5-single-levels',\n",
    "        {\n",
    "            'product_type': 'reanalysis',\n",
    "            'format': 'netcdf',\n",
    "            'variable': [\n",
    "                '10m_u_component_of_wind','10m_v_component_of_wind'\n",
    "            ],\n",
    "            'year': year,\n",
    "            'month': [\n",
    "                '01', '02', '03',\n",
    "                '04', '05', '06',\n",
    "                '07', '08', '09',\n",
    "                '10', '11', '12',\n",
    "            ],\n",
    "            'day': [\n",
    "                '01', '04', '07',\n",
    "                '10', '13', '16',\n",
    "                '19', '22', '25',\n",
    "                '28',\n",
    "            ],\n",
    "            'time': [\n",
    "                '00:00', '05:00', '10:00',\n",
    "                '15:00', '20:00',\n",
    "            ],\n",
    "            'area': [\n",
    "                31, -91, 18,\n",
    "                -80,\n",
    "            ],\n",
    "        },\n",
    "        f\"WINDyear{year}.nc\")\n",
    "    \n",
    "    shutil.move(f\"WINDyear{year}.nc\", destination_dir)  #these files are not zip, so no need to delete after like in previous code       \n",
    "        \n",
    "    print(f\"Download for year {year} complete\")\n",
    "print('All downloads complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1844e-b299-45c0-a6d4-64a6374090f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = xr.open_mfdataset('SSTdata/*.nc', parallel=True) #access all SST files as one variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c395f999-b44b-4c08-9a65-c6e569069d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reducing global data down to study area\n",
    "gomslice = ds.analysed_sst.sel(\n",
    "    lat=slice(18,31),\n",
    "    lon=slice(-91,-80))\n",
    "gomslice.load().to_netcdf('SSTdata/GoMSSTdata.nc')   #saving as new file. At this point, the previous nc files can be deleted if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f094f-ac2c-4d1b-92db-544f4c14e5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('SSTdata/GoMSSTdata.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2588c-ff76-43a2-8d97-b24566229d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Converting from Kelvin to degrees Celsius and calculating SST anomaly\n",
    "\n",
    "ds['analysed_sst'] = ds['analysed_sst'] - 273.15       #Kelvin to Celsius\n",
    "monthly_mean_sst = ds.analysed_sst.groupby('time.month').mean(dim='time')   #Making new dataset with only monthly mean SST    \n",
    "ssta = ds.analysed_sst.groupby('time.month')-monthly_mean_sst               #Subtracting monthly mean dataset from ds to obtain anomaly\n",
    "ds = ssta.to_dataset()\n",
    "ds = ds.rename({'analysed_sst':'ssta'})\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66181d-6626-40d6-af95-bfc5f3b657e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plotting to observe study area\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ds['ssta'][0,:,:].plot(ax=ax)\n",
    "ax.coastlines()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31004d4e-d85d-4442-8577-4d1763e4db02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#converting data time values to weekly for comparison with Weisberg et al. 2014\n",
    "end_date = '2010-12-31'\n",
    "weekly_dates = pd.date_range(start=ds.time[0].values, end=end_date, freq='W') #generating weekly dates from the first date in data to '2010-12-31\n",
    "ds_weekly = ds.interp(time=weekly_dates).copy()\n",
    "ds_weekly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e189f-51c3-4669-baeb-09864071ff87",
   "metadata": {},
   "source": [
    "Generating Trackline 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f04d4c-e3f9-4cca-ae9f-851f9e3bc136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trackline code originally referenced from exercise provided by instructor\n",
    "# This code is generated by ChatGPT 3.5\n",
    "# Define the start and end points\n",
    "start_point = (-85.79403277044844, 28.44874852567387)\n",
    "end_point = (-83.61900543369907, 23.84111494192613)\n",
    "\n",
    "# Calculate the distance between the two points using Vincenty's formulae\n",
    "def vincenty_distance(lat1, lon1, lat2, lon2):\n",
    "    earth_radius = 6371  # Earth's radius in kilometers\n",
    "    phi1 = math.radians(lat1)\n",
    "    phi2 = math.radians(lat2)\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2 - lon1)\n",
    "\n",
    "    a = math.sin(delta_phi/2) * math.sin(delta_phi/2) + math.cos(phi1) * math.cos(phi2) * math.sin(delta_lambda/2) * math.sin(delta_lambda/2)\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    distance = earth_radius * c\n",
    "\n",
    "    return distance\n",
    "\n",
    "total_distance = vincenty_distance(start_point[1], start_point[0], end_point[1], end_point[0])\n",
    "\n",
    "# Calculate intermediate points and store them in a NumPy array\n",
    "num_points = 100\n",
    "intermediate_points = np.zeros((num_points, 2))  # Initialize a NumPy array to store the points\n",
    "\n",
    "for i in range(num_points):\n",
    "    ratio = (i + 1) / (num_points + 1)\n",
    "    intermediate_lat = start_point[0] + (end_point[0] - start_point[0]) * ratio\n",
    "    intermediate_lon = start_point[1] + (end_point[1] - start_point[1]) * ratio\n",
    "    intermediate_points[i] = [intermediate_lat, intermediate_lon]\n",
    "\n",
    "print('intermediate_points has shape', intermediate_points.shape)\n",
    "print('Start point',intermediate_points[0])\n",
    "print('End point',intermediate_points[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a998a3-1584-40f7-b52a-2561fddddd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make coordinate of 100 points aling trackine 26 consistant with SLA data\n",
    "points= intermediate_points.copy()\n",
    "points[:, [0, 1]] = points[:, [1, 0]] # Swap the columns\n",
    "print(points.shape)\n",
    "print('latitudes, longitudes')\n",
    "print(points[0])\n",
    "print(points[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0396ca-6b40-48fd-ba2e-171ada82dd49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Revealing trackline 26 on a plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ds['ssta'][0,:,:].plot(ax=ax)\n",
    "ax.plot(intermediate_points[:,0],intermediate_points[:,1], color='black', linewidth=2, zorder = 15)\n",
    "ax.text(0.535, 0.6, 'Track 026', transform=ax.transAxes, \n",
    "        color='black', fontsize=10,  ha='center',  va='center',  rotation=300, zorder = 15)\n",
    "ax.coastlines()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e18492-84b2-4111-8162-e8a4296f1518",
   "metadata": {},
   "source": [
    "Extracting Data Along Trackline 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff4b8e-8874-4fa9-83ee-02c11527d425",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = ds_weekly.time\n",
    "latitude = points[:, 0]\n",
    "longitude = points[:, 1]\n",
    "\n",
    "# Define the size of your data\n",
    "size = (len(time), len(latitude))\n",
    "\n",
    "# Create NaN-filled array for sea surface temperature\n",
    "sst = np.full(size, np.nan) \n",
    "\n",
    "# Create a DataArray with sst values\n",
    "sst_data_array = xr.DataArray(\n",
    "    sst,\n",
    "    dims=('time', 'latitude'),\n",
    "    coords={'time': time, 'latitude': latitude},\n",
    "    name='sst'\n",
    ")\n",
    "\n",
    "# Create a Dataset with sst variable\n",
    "ds_TL26 = xr.Dataset({'sst': sst_data_array})\n",
    "\n",
    "# Add longitude coordinate\n",
    "ds_TL26.coords['longitude'] = ('latitude', longitude)\n",
    "\n",
    "# Add metadata to the DataArray\n",
    "ds_TL26.sst.attrs['Long Name'] = 'sea_surface_temperature'\n",
    "ds_TL26.sst.attrs['Short Name'] = 'sst'\n",
    "ds_TL26.sst.attrs['Units'] = 'Celsius'\n",
    "ds_TL26.sst.attrs['Description'] = \"Sea surface temperature along trackline 26 \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f7989-24f0-47b6-bcd0-cc1cde4363b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You can create a grid of coordinates for which you want to interpolate values\n",
    "time_coords = ds_TL26.time\n",
    "latitude_coords = ds_TL26.latitude\n",
    "longitude_coords = ds_TL26.longitude\n",
    "\n",
    "# Use meshgrid to create a full grid of (time, latitude, longitude)\n",
    "time_grid, lat_grid, lon_grid = xr.broadcast(time_coords, latitude_coords, longitude_coords)\n",
    "\n",
    "# Interpolate using the interp method over the entire grid\n",
    "interpolated_ds = ds.analysed_sst.interp(time=time_grid, lat=lat_grid, lon=lon_grid, method='linear')\n",
    "\n",
    "# Now interpolated_ds contains the interpolated values for the entire grid\n",
    "# Assuming that the original shape of ds_TL26.sla is compatible, you can assign the values directly\n",
    "ds_TL26['sst'] = interpolated_ds\n",
    "\n",
    "ds_TL26.sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433ff54-d44f-4f33-8c6e-8d08993f834a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a grid of time and latitude values for contour plot\n",
    "lat_grid, time_grid = np.meshgrid(ds_TL26.latitude,ds_TL26.time,)\n",
    "\n",
    "# Create a Figure and Axes object\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Create the contour plot\n",
    "contour = ax.contourf(time_grid,lat_grid, ds_TL26.sst, cmap='jet')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(contour, ax=ax, orientation='horizontal',shrink=0.6)\n",
    "cbar.set_label('Sea Surface Temperature Anomaly (C)')\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel('Time (year)')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "# Format x-axis tick labels to display only last two digits of the year\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Set x-axis major locator and formatter to show ticks and labels for every year\n",
    "years = mdates.YearLocator()\n",
    "ax.xaxis.set_major_locator(years)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%y'))  # Use '%y' to display only the last two digits of the year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af6633-d04b-4950-a06e-6a23bf037d33",
   "metadata": {},
   "source": [
    "K. Brevis cell count data sourced from the National Centers for Environmental Infomration (NCEI):\n",
    "https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.nodc:0120767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15a914-13c0-49b4-8b4b-77c10610188c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#After downloading KBrevis CSV data from above and renaming to 'kbrevis.csv'\n",
    "\n",
    "df = pd.read_csv('kbrevis.csv', low_memory=False)\n",
    "columns = ['STATE_ID', 'LATITUDE', 'LONGITUDE', 'SAMPLE_DATE', 'CELLCOUNT']\n",
    "df=df[columns]\n",
    "#selecting all data collected in Florida\n",
    "\n",
    "fl_mask = (df['STATE_ID'] == 'FL')\n",
    "df = df[fl_mask]\n",
    "#selecting all data collected from Naples to Tampa Bay based on latitude\n",
    "\n",
    "west_coast_mask = ((df['LATITUDE'] >= 26) & (df['LATITUDE'] <= 28))\n",
    "df = df[west_coast_mask]\n",
    "#converting sample date to datetime format and setting as index\n",
    "\n",
    "df['SAMPLE_DATE'] = pd.to_datetime(df['SAMPLE_DATE'])\n",
    "df.set_index('SAMPLE_DATE', inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "#matching start and end dates with sst data\n",
    "\n",
    "start_date = '1997-01-01'\n",
    "end_date = '2011-01-01'\n",
    "df = df[start_date:end_date]\n",
    "\n",
    "Kbrevis = df['CELLCOUNT'].resample('W').mean()\n",
    "Kbrevis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0677ab72-b1cd-4a77-a983-a1866fc105a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plotting SST Anomaly contour with Kbrevis data\n",
    "fig, ax = plt.subplots(2, 1, sharex=True, figsize=(14, 8))\n",
    "ax[0].plot(Kbrevis, label = 'West Florida K. brevis')\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_ylim(1e5, None)\n",
    "ax[0].set_ylabel('Cell Count (Cells/L)')\n",
    "\n",
    "years = mdates.YearLocator()\n",
    "ax[0].xaxis.set_major_locator(years)\n",
    "ax[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "lat_grid, time_grid = np.meshgrid(ds_TL26.latitude,ds_TL26.time)\n",
    "\n",
    "# Create the contour plot\n",
    "contour = ax[1].contourf(time_grid,lat_grid, ds_TL26.sst, cmap='jet')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(contour, ax=ax, orientation='horizontal',shrink=0.6)\n",
    "cbar.set_label('Sea Surface Temperature Anomaly (C)')\n",
    "\n",
    "# Set labels and title\n",
    "ax[1].set_xlabel('Time (year)')\n",
    "ax[1].set_ylabel('Latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b835852-b768-47d8-98e1-ac01b782a9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Separating SST anomaly data along trackline into one-latitude chunks to find region with best correlation to K. brevis\n",
    "target_lats = [24, 25, 26, 27, 28]\n",
    "for upper_lat in target_lats:\n",
    "    \n",
    "    #obtains index of the value closest to the target latitude\n",
    "    idx_target = np.argmin(np.abs(ds_TL26.latitude.values - upper_lat))\n",
    "    \n",
    "    #obtains index of the value closest to one minus the target latitude to obtain ~1-latitude ranges\n",
    "    idx_start = np.argmin(np.abs(ds_TL26.latitude.values - upper_lat -1))\n",
    "    \n",
    "    #slicing by the found indices\n",
    "    ds_TL26_LowLat = ds_TL26.isel(latitude=slice(idx_start, idx_target)).copy()  \n",
    "    \n",
    "    #plotting\n",
    "    fig, ax= plt.subplots(figsize=(8, 2))\n",
    "    lat_grid, time_grid = np.meshgrid(ds_TL26_LowLat.latitude,ds_TL26_LowLat.time,)\n",
    "    contour = ax.contourf(time_grid, lat_grid, ds_TL26_LowLat.sst, cmap='jet')\n",
    "    cbar = plt.colorbar(contour, ax=ax)\n",
    "    cbar.set_label('SST Anomaly (C)')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Latitude')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967056d-4498-404a-955a-9893832e8234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Alternative visualization with just SST anomaly below zero\n",
    "fig, ax = plt.subplots(6, 1, figsize = (12,15))\n",
    "ax[0].plot(Kbrevis, label = 'West Florida K. brevis', color='red')\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_ylim(1e5, None)\n",
    "ax[0].set_ylabel('Cell Count (Cells/L)')\n",
    "ax[0].legend()\n",
    "\n",
    "years = mdates.YearLocator()\n",
    "ax[0].xaxis.set_major_locator(years)\n",
    "ax[0].xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "target_lats = [24, 25, 26, 27, 28]\n",
    "for i, upper_lat in enumerate (target_lats):\n",
    "    idx_target = np.argmin(np.abs(ds_TL26.latitude.values - upper_lat))\n",
    "    idx_start = np.argmin(np.abs(ds_TL26.latitude.values - upper_lat -1))\n",
    "    ds_TL26_LowLat = ds_TL26.isel(latitude=slice(idx_start, idx_target)).copy()\n",
    "    \n",
    "    minlat = ds_TL26_LowLat['latitude'].min().values\n",
    "    maxlat = ds_TL26_LowLat['latitude'].max().values    #obtaining min and max lat values for labeling legend\n",
    "    \n",
    "    ds_TL26_LowLat_mean = ds_TL26_LowLat.sst.mean(dim='latitude').copy()    #collapses latitude dimension\n",
    "    ds_TL26_LowLat_mean.to_dataset()\n",
    "    #selecting all cold values (SST anomaly less than 0, or else SST anomaly is 0)\n",
    "    ds_TL26_LowLat_mean = ds_TL26_LowLat_mean.where(ds_TL26_LowLat_mean < 0, 0) \n",
    "    \n",
    "    #Plotting\n",
    "    ax[i+1].plot(ds_TL26_LowLat.time, ds_TL26_LowLat_mean, label = \n",
    "                 f\"Latitude {round(float(minlat), 2)} - {round(float(maxlat), 2)}\")  #labeling each subplot with min and max lat\n",
    "    ax[i+1].legend()\n",
    "    ax[i+1].xaxis.set_major_locator(years)\n",
    "    ax[i+1].xaxis.set_major_formatter(mdates.DateFormatter('%Y')) #setting yearly x-axis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64d6bd0-bb68-4328-b0f5-1bbc45f497fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loop for calculating correlations by latitude chunk, but values are very low\n",
    "Kbrevis = Kbrevis.fillna(0)              #filling NaN values with 0\n",
    "target_lats = [24, 25, 26, 27, 28]\n",
    "for i, upper_lat in enumerate (target_lats):\n",
    "    idx_target = np.argmin(np.abs(ds_TL26.latitude.values - upper_lat))           \n",
    "    idx_start = np.argmin(np.abs(ds_TL26.latitude.values - upper_lat -1))\n",
    "    ds_TL26_LowLat = ds_TL26.isel(latitude=slice(idx_start, idx_target)).copy()\n",
    "    ds_TL26_LowLat = ds_TL26_LowLat.sel(time=slice('1997-01-01', '2009-12-31'))\n",
    "    \n",
    "    Kbrevis = Kbrevis.loc['1997-01-01':'2009-12-31'] \n",
    "                                        \n",
    "    minlat = ds_TL26_LowLat['latitude'].min().values\n",
    "    maxlat = ds_TL26_LowLat['latitude'].max().values\n",
    "    \n",
    "    ds_TL26_LowLat_mean = ds_TL26_LowLat.sst.mean(dim='latitude').copy()\n",
    "    ds_TL26_LowLat_mean.to_dataset()\n",
    "    #selecting all cold values (SST anomaly less than 0, or else SST anomaly = 0)\n",
    "    ds_TL26_LowLat_mean = ds_TL26_LowLat_mean.where(ds_TL26_LowLat_mean < 0, 0)   \n",
    "\n",
    "    corr, p_value = pearsonr(ds_TL26_LowLat_mean, Kbrevis)\n",
    "    \n",
    "    print(f\"Correlation coefficient for latitudes {minlat} - {maxlat}: {corr}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0681da7e-f172-438a-a2c8-35af0545fff2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "windds = xr.open_mfdataset('WINDdata/*.nc', parallel = True)  #opening all wind nc files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5288a4-8204-4e72-8e89-c27bd520fd12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Resampling wind data to monthly mean \n",
    "new_wind = windds.sel(time = slice('1997-01-01', '2010-12-26'))    \n",
    "new_wind = windds.resample(time='1M').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd9dc1-22e8-46d4-9fbd-fbf8797d8b2c",
   "metadata": {},
   "source": [
    "Upwelling Index equations derived from:\n",
    "Jayaram, Chiranjivi, and Felix Jose. “Relative Dominance of Wind Stress Curl and Ekman Transport on Coastal Upwelling During Summer Monsoon in the Southeastern Arabian Sea.” Continental Shelf Research, vol. 244, Elsevier BV, July 2022, p. 104782. Crossref, https://doi.org/10.1016/j.csr.2022.104782."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f45d07-fe78-4c01-b10f-4d7055a6c44a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Calculating upwelling index\n",
    "P = 1.225 #kg/m^3  air density\n",
    "Pw = 1024 #kg/m^3  seawater density\n",
    "Cd = 2.6 * 10**-3 #drag coefficient for moderate winds\n",
    "f = 0.622 * 10**-4 #s^-1, coriolis parameter\n",
    "def upwelling_index (u, v):\n",
    "    W = np.sqrt(u**2 + v**2)                                          #calculating net wind\n",
    "    thetaW = (180-155.5)+(np.degrees(np.arctan(W)))-90                #angle of the wind with respect to the Florida shoreline\n",
    "    Ushorecomp = [(u*np.cos(thetaW-180))+(v*np.sin(thetaW-180))]      #component of wind parallel to the shoreline\n",
    "    t = P*Cd*np.abs(W)*Ushorecomp                                     #along shore compnent of wind shear stress\n",
    "    \n",
    "    EkmanTransport = t/(Pw*f)                                         #Ekman Transport equation (m^3 per second)\n",
    "    \n",
    "    return EkmanTransport\n",
    "    \n",
    "#Using upwelling index function to fill new variable in wind dataset with the ekman transport values\n",
    "EkmTrans_data = upwelling_index(new_wind['u10'].values, new_wind['v10'].values)       # (u, v)  \n",
    "EkmTrans_data = np.squeeze(EkmTrans_data, axis = 0)\n",
    "EkmTrans_data = xr.DataArray(EkmTrans_data, dims=['time','latitude', 'longitude'], name='EkmTrans')\n",
    "\n",
    "new_wind['EkmTrans'] = EkmTrans_data  #new variable 'EkmTrans' filled with calculated value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728c1ad-aab4-4c82-a189-de626caa1084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#making a copy of the SST anomaly dataset to resampled to monthly mean to plot together with the wind data\n",
    "dsmonthly = ds.resample(time = '1M').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9671bcd-39d2-40a2-aee3-58b316a564cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Plotting maps of SST anomaly overlayed with wind direction quivers\n",
    "dates = ['2003-01-31','2004-01-31','2005-01-31', '2006-01-31', '2007-01-31', '2008-01-31','2009-01-31', '2010-01-31', '2011-01-31']\n",
    "\n",
    "longitude = new_wind['longitude'].values\n",
    "latitude = new_wind['latitude'].values         #lon, lat for input into np.meshgrid\n",
    "\n",
    "lon, lat = np.meshgrid(longitude, latitude)\n",
    "\n",
    "extent = [-90, -79, 23.5, 31]                 #map extent\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 3, ncols = 3 , figsize=(10, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "for i, date in enumerate(dates):\n",
    "    ax = axes.flat[i]\n",
    "    #plotting monthly SST anomaly, reduce 'shrink' if colorbars are too large\n",
    "    dsmonthly.sel(time=date, method='nearest')['ssta'].plot(ax=ax, cbar_kwargs={'shrink': 1, 'label': 'SST Anomaly'})\n",
    "    \n",
    "    #setup for quiver function \n",
    "    u10 = new_wind['u10'].sel(time=date, method='nearest').values\n",
    "    v10 = new_wind['v10'].sel(time=date, method='nearest').values\n",
    "    \n",
    "    stride = 5 # Adjust stride as needed to reduce the number of quivers\n",
    "    lon_downsampled = lon[::stride, ::stride]\n",
    "    lat_downsampled = lat[::stride, ::stride]\n",
    "    u10_downsampled = u10[::stride, ::stride]\n",
    "    v10_downsampled = v10[::stride, ::stride]\n",
    "    \n",
    "    #quiver function (lon, lat, u, v)\n",
    "    ax.quiver(lon_downsampled, lat_downsampled, u10_downsampled, v10_downsampled, transform=ccrs.PlateCarree())\n",
    "    ax.coastlines()\n",
    "    ax.set_extent(extent)\n",
    "    ax.set_title(f\"{date}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('SSTvectormaps.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda34ea-ed38-4a27-9f0a-c7e6ef2f44ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Same code as above except plotting Ekman Transport values\n",
    "dates = ['2003-01-31','2004-01-31','2005-01-31', '2006-01-31', '2007-01-31', '2008-01-31','2009-01-31', '2010-01-31', '2011-01-31']\n",
    "\n",
    "longitude = new_wind['longitude'].values\n",
    "latitude = new_wind['latitude'].values\n",
    "\n",
    "lon, lat = np.meshgrid(longitude, latitude)\n",
    "\n",
    "extent = [-90, -80, 23.5, 31]\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 3, ncols = 3 , figsize=(10, 6), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "\n",
    "\n",
    "for i, date in enumerate(dates):\n",
    "    ax = axes.flat[i]\n",
    "    new_wind.sel(time=date, method='nearest')['EkmTrans'].plot(ax=ax, cmap='seismic', cbar_kwargs={'label': 'Upwelling Index'})\n",
    "    \n",
    "    u10 = new_wind['u10'].sel(time=date, method='nearest').values\n",
    "    v10 = new_wind['v10'].sel(time=date, method='nearest').values\n",
    "    \n",
    "    stride = 5 # Adjust stride as needed to reduce the number of quivers\n",
    "    lon_downsampled = lon[::stride, ::stride]\n",
    "    lat_downsampled = lat[::stride, ::stride]\n",
    "    u10_downsampled = u10[::stride, ::stride]\n",
    "    v10_downsampled = v10[::stride, ::stride]\n",
    "    \n",
    "    ax.quiver(lon_downsampled, lat_downsampled, u10_downsampled, v10_downsampled, transform=ccrs.PlateCarree())\n",
    "    ax.coastlines()\n",
    "    ax.set_extent(extent)\n",
    "    ax.set_title(f\"{date}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "#plt.savefig('UIvectormaps.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
